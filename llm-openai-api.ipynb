{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["n8Ok5yClYVg-","Mf2I2Ev8YZ6X","noqBFr4bZ33d","UhnbZxZyh7Am","4m4ZFjUviR79"],"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ca4564b5a43942bd965b71c95d590597":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9bf870657ad4f29b367fab27d5c2b05","IPY_MODEL_95f8b103f2bf4464b4d1c4d118d39ba3","IPY_MODEL_001019753f6248fa986d63a83d1d5c41"],"layout":"IPY_MODEL_76fef40758f347749cd41e4c5f5c6806"}},"a9bf870657ad4f29b367fab27d5c2b05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4103a592f7ce486881c92df0b1b4bf59","placeholder":"​","style":"IPY_MODEL_854cc2b642ca48b39b819d5abd5a2325","value":"model.safetensors: 100%"}},"95f8b103f2bf4464b4d1c4d118d39ba3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_63d15564355641c0ba483f874efb4c9d","max":2264298476,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1f2d05c25314520a536f26c1c19d91b","value":2264298261}},"001019753f6248fa986d63a83d1d5c41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fca0cdd7d374c038a143e45fcc4389a","placeholder":"​","style":"IPY_MODEL_c4df12f6133245a8bba7ec3a5de8c961","value":" 2.26G/2.26G [00:21&lt;00:00, 328MB/s]"}},"76fef40758f347749cd41e4c5f5c6806":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4103a592f7ce486881c92df0b1b4bf59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"854cc2b642ca48b39b819d5abd5a2325":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63d15564355641c0ba483f874efb4c9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f2d05c25314520a536f26c1c19d91b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1fca0cdd7d374c038a143e45fcc4389a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4df12f6133245a8bba7ec3a5de8c961":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ecccaefd21d4d859602dad6b9d125a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e62d7ad9172a4729a9ec542799538abf","IPY_MODEL_0c40b87805864dae85271a2bfb796b4b","IPY_MODEL_b2032d9858d246e99a0ea29e2f1efde3"],"layout":"IPY_MODEL_a1b816f74f1a485f85a625aff9e4bacb"}},"e62d7ad9172a4729a9ec542799538abf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d50128505764b23b9b5d22d52a97d31","placeholder":"​","style":"IPY_MODEL_df1cdd4590e44529aa8d29e06dbe0bb8","value":"generation_config.json: 100%"}},"0c40b87805864dae85271a2bfb796b4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35893825855413b9656aec3cd121cb6","max":140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43986b4278414592aba2e451c9641341","value":140}},"b2032d9858d246e99a0ea29e2f1efde3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5928858b28ad49c19565e063be6c560e","placeholder":"​","style":"IPY_MODEL_52af6bc3ca3b40288523cdf78381cb2c","value":" 140/140 [00:00&lt;00:00, 4.86kB/s]"}},"a1b816f74f1a485f85a625aff9e4bacb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d50128505764b23b9b5d22d52a97d31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df1cdd4590e44529aa8d29e06dbe0bb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a35893825855413b9656aec3cd121cb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43986b4278414592aba2e451c9641341":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5928858b28ad49c19565e063be6c560e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52af6bc3ca3b40288523cdf78381cb2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b20b8fe06ece4914a5553dd51c37acd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4ce2532cf084b46b168a40a0f5d15ed","IPY_MODEL_8e160d276dbf431d9e4ce7f8e2518449","IPY_MODEL_6784362e34eb43db9804d2dd45524919"],"layout":"IPY_MODEL_abf2c721029443ffabddd0d4fc9a4da7"}},"b4ce2532cf084b46b168a40a0f5d15ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279d537488d04891ac54b1d675229a59","placeholder":"​","style":"IPY_MODEL_6d3c1527050442e48c66a2c42080f2a4","value":"tokenizer_config.json: 100%"}},"8e160d276dbf431d9e4ce7f8e2518449":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9bbf62252af4773b107d10e78473afa","max":3367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e38298ed4b074caaa6f7c27f14be0fce","value":3367}},"6784362e34eb43db9804d2dd45524919":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a76113a3890847a0ad85bbdfe0def196","placeholder":"​","style":"IPY_MODEL_870322f0245e4d9292a6465de63c1921","value":" 3.37k/3.37k [00:00&lt;00:00, 258kB/s]"}},"abf2c721029443ffabddd0d4fc9a4da7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279d537488d04891ac54b1d675229a59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3c1527050442e48c66a2c42080f2a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9bbf62252af4773b107d10e78473afa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e38298ed4b074caaa6f7c27f14be0fce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a76113a3890847a0ad85bbdfe0def196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"870322f0245e4d9292a6465de63c1921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"180bd301be244be6bbe8c8918d604af3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_819eabbaf741480fb81f41fffeb501f6","IPY_MODEL_d93ed850b38e4ccab33ce9500125dcd6","IPY_MODEL_b055c16044e54386b3183b39c42ba799"],"layout":"IPY_MODEL_c63f178a66314f899b1ae18476ffdae3"}},"819eabbaf741480fb81f41fffeb501f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf3526fbf914e098afb230901561488","placeholder":"​","style":"IPY_MODEL_8c09fc35efa143a9a11525b395a3f88a","value":"tokenizer.model: 100%"}},"d93ed850b38e4ccab33ce9500125dcd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_981aef0d94534de6a82075613a922003","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b114a7755a040beaeda35eef5c0da72","value":499723}},"b055c16044e54386b3183b39c42ba799":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03ad52d8f3ef4b718e89769b5108c94d","placeholder":"​","style":"IPY_MODEL_a747d2a12131409d878c3c4cc7b7784c","value":" 500k/500k [00:00&lt;00:00, 9.14MB/s]"}},"c63f178a66314f899b1ae18476ffdae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cf3526fbf914e098afb230901561488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c09fc35efa143a9a11525b395a3f88a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"981aef0d94534de6a82075613a922003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b114a7755a040beaeda35eef5c0da72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03ad52d8f3ef4b718e89769b5108c94d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a747d2a12131409d878c3c4cc7b7784c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5b11c66daf240458a57dc3ffbe2af68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b10a9074dc6e40f1b0c5c95924c9fda4","IPY_MODEL_aae76f6904a143418f543f203157fcf2","IPY_MODEL_dfd6072be8564dfa90aa855dd13b102b"],"layout":"IPY_MODEL_ec3299f897314caf9c311aed4f2db96e"}},"b10a9074dc6e40f1b0c5c95924c9fda4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86c7120aa89f4a6088c8c6c2d76c9433","placeholder":"​","style":"IPY_MODEL_f08d7b649454463aaafc8aca72c8d33b","value":"added_tokens.json: 100%"}},"aae76f6904a143418f543f203157fcf2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b0d26ade751478cb945fc1fc3f135a4","max":293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_979877e9ba5146e7a5f44712f7c097e8","value":293}},"dfd6072be8564dfa90aa855dd13b102b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83c6ef2f156c497bb798ced4788c04bc","placeholder":"​","style":"IPY_MODEL_50d877d0dc2749d28182fbdb1feb995a","value":" 293/293 [00:00&lt;00:00, 19.5kB/s]"}},"ec3299f897314caf9c311aed4f2db96e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86c7120aa89f4a6088c8c6c2d76c9433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08d7b649454463aaafc8aca72c8d33b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b0d26ade751478cb945fc1fc3f135a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"979877e9ba5146e7a5f44712f7c097e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83c6ef2f156c497bb798ced4788c04bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d877d0dc2749d28182fbdb1feb995a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"983862c1baa846628d10626f5a155f23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90f6a316e81e4cbb8acbc19bc6ca5d24","IPY_MODEL_e49bf33368a34567ad49252465c08254","IPY_MODEL_c75d620fb57f444c85a19b70656868a6"],"layout":"IPY_MODEL_402b33690b224bd39378de95217eec4c"}},"90f6a316e81e4cbb8acbc19bc6ca5d24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3b6779ff724132b269f041ef3f3e8f","placeholder":"​","style":"IPY_MODEL_63b261dc523a428293629e96131ec27b","value":"special_tokens_map.json: 100%"}},"e49bf33368a34567ad49252465c08254":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e4ee804eb2c4e7ea04d163788feeeb5","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_088669a32de34146a35628928a65d89e","value":571}},"c75d620fb57f444c85a19b70656868a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a5ab00f7c044526b743dbe5ac6a74d6","placeholder":"​","style":"IPY_MODEL_d9e760c2582c482087e1beca1a619808","value":" 571/571 [00:00&lt;00:00, 37.6kB/s]"}},"402b33690b224bd39378de95217eec4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a3b6779ff724132b269f041ef3f3e8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63b261dc523a428293629e96131ec27b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e4ee804eb2c4e7ea04d163788feeeb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"088669a32de34146a35628928a65d89e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a5ab00f7c044526b743dbe5ac6a74d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9e760c2582c482087e1beca1a619808":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecfd957a14414138a6e9c982f77a6a91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5614963e84d34a309b859fa79f206185","IPY_MODEL_01973e310c2842e1b031167bc7db6a80","IPY_MODEL_aaaa6f8f6df64842b134d95a0466fcec"],"layout":"IPY_MODEL_b7badbc284744462bd4f06643d5365e5"}},"5614963e84d34a309b859fa79f206185":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e71684a896d545acaced19f451350eb0","placeholder":"​","style":"IPY_MODEL_346f7421e84f4ef29988a63005077afa","value":"tokenizer.json: 100%"}},"01973e310c2842e1b031167bc7db6a80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_683478ba40a04e0788c827eb70a793d1","max":1844436,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0650820a3d144f4ae73dca45b41dca4","value":1844436}},"aaaa6f8f6df64842b134d95a0466fcec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec62ac2e2014d9c8a0bf2b320083778","placeholder":"​","style":"IPY_MODEL_59ea8fe6a0d74cc8986dbd9917e2eaac","value":" 1.84M/1.84M [00:00&lt;00:00, 2.12MB/s]"}},"b7badbc284744462bd4f06643d5365e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e71684a896d545acaced19f451350eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"346f7421e84f4ef29988a63005077afa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"683478ba40a04e0788c827eb70a793d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0650820a3d144f4ae73dca45b41dca4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ec62ac2e2014d9c8a0bf2b320083778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59ea8fe6a0d74cc8986dbd9917e2eaac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/antonchernov/llm-openai-api?scriptVersionId=196992858\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Installs","metadata":{"id":"iCC7uAaiYX-Z"}},{"cell_type":"code","source":"%%capture\n\n!pip install flask-localtunnel\n!pip install flask\n!pip install flask-pydantic\n\n# !pip install transformers\n!pip install uvicorn","metadata":{"id":"3_rd2lcIMuBl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# !pip install streamlit\n\n!pip install uvicorn\n!npm install -g localtunnel\n\n!pip install fastapi\n!pip install fastapi[standard]\n!pip install sse-starlette","metadata":{"id":"bzGfiGDYwXdO","executionInfo":{"status":"ok","timestamp":1725533326022,"user_tz":-180,"elapsed":34155,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"execution":{"iopub.status.busy":"2024-09-09T11:32:29.814462Z","iopub.execute_input":"2024-09-09T11:32:29.815034Z","iopub.status.idle":"2024-09-09T11:33:30.431145Z","shell.execute_reply.started":"2024-09-09T11:32:29.814995Z","shell.execute_reply":"2024-09-09T11:33:30.429613Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Streamlit basic demo","metadata":{"id":"n8Ok5yClYVg-"}},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nfrom transformers import pipeline\nimport time\n\n# Load your model (with caching to improve performance)\n@st.cache_resource()\ndef load_model():\n    model = pipeline('text-generation', model='gpt2')\n    return model\n\nmodel = load_model()\n\n# Set up your web app\nst.title('GPT-2 Story Completer')\nst.header('Enter text to complete:')\n\nuser_input = st.text_area('Write something to activate the AI:', height=200)\nmax_length = st.slider(\"Select max story length:\", min_value=50, max_value=200, value=100, step=10)\nnum_sequences = st.selectbox(\"Select number of stories to generate:\", options=[1, 2, 3], index=0)\n\nif st.button('Generate Story'):\n    with st.spinner('Generating Story...'):\n        response = model(user_input, max_length=max_length, num_return_sequences=num_sequences)\n        for i, summary in enumerate(response):\n            st.write(f'**Story {i+1}:**')\n            st.write(summary['generated_text'])\n            st.markdown(\"---\")\n\nst.sidebar.markdown(\"## Guide\")\nst.sidebar.info(\"This tool uses GPT-2 to generate a story of your provided text. Adjust the sliders to change the story length and number of stories generated. The model is optimized for short to medium length paragraphs.\")\nst.sidebar.markdown(\"### Examples\")\nst.sidebar.write(\"1. Paste a beginning to see how the AI completes it.\")\nst.sidebar.write(\"2. Try different settings to see how the story changes.\")\n#Note : You may need to add HF_TOKEN in Colab Secrets depending on the LLM. You can access your tokens from https://huggingface.co/settings/tokens","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1725390674460,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"mKaT9odFSAyI","outputId":"4864d287-74ec-4204-c548-71a102b4409e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl ipv4.icanhazip.com\n!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72200,"status":"ok","timestamp":1725390747447,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"WxVpzN8DSH1q","outputId":"952de57e-fa61-4faf-b3d6-afdf4e0477b6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FastAPI basic demo","metadata":{"id":"Mf2I2Ev8YZ6X"}},{"cell_type":"code","source":"%%writefile main.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1725517189759,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"WhL7dHEwYb_p","outputId":"5ba0ade9-e0c0-4cbf-9bba-40cdbf4f45c3","execution":{"iopub.status.busy":"2024-09-08T07:28:55.473547Z","iopub.execute_input":"2024-09-08T07:28:55.47399Z","iopub.status.idle":"2024-09-08T07:28:55.480363Z","shell.execute_reply.started":"2024-09-08T07:28:55.473948Z","shell.execute_reply":"2024-09-08T07:28:55.479252Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"PORT = 8081\n\n!curl ipv4.icanhazip.com\n!uvicorn main:app --host 0.0.0.0 --port {PORT} & lt --port {PORT}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36894,"status":"ok","timestamp":1725517226649,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"spN27CQnZfZb","outputId":"13c3cdb8-af57-4e2d-9979-76a97545a9ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FastAPI GPT demo","metadata":{"id":"noqBFr4bZ33d"}},{"cell_type":"code","source":"#@title Simple app\n\n%%writefile main.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Define the request model\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int\n\n# Define a POST endpoint that returns a static string\n@app.post(\"/v1/completions\")\nasync def generate_completion(request: CompletionRequest):\n    # Instead of generating text, just return a static response\n    return {\"completion\": f\"Received prompt: {request.prompt}, max_tokens: {request.max_tokens}\"}\n","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1725516089767,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"Oub0LlHhbi0q","outputId":"84b46667-1819-4edc-b7bb-3c5f1f40358e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Completion GPT-2 API\n\n%%writefile main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Load the Hugging Face GPT-2 model\nmodel = pipeline(\"text-generation\", model=\"gpt2\")\n\n# Define the request model\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.post(\"/v1/completions\", response_model=CompletionResponse)\nasync def generate_completion(request: CompletionRequest):\n    try:\n        # Generate the text completion\n        completions = model(request.prompt, max_length=request.max_tokens, num_return_sequences=1)\n        completion_text = completions[0][\"generated_text\"]\n        return CompletionResponse(completion=completion_text)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1725516358597,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"2gGUEL8Cab8d","outputId":"f161c0cd-8312-4693-8894-e6775c742edc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Embeddings API\n\n%%writefile main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Load BERT emb_tokenizer and emb_model (a small BERT emb_model)\nemb_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nemb_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Define the request emb_model\nclass EmbeddingRequest(BaseModel):\n    text: str\n\n# Define the response emb_model\nclass EmbeddingData(BaseModel):\n    object: str\n    embedding: list\n    index: int\n\nclass EmbeddingResponse(BaseModel):\n    object: str\n    data: list[EmbeddingData]\n    emb_model: str\n\n@app.post(\"/v1/embeddings\", response_model=EmbeddingResponse)\nasync def generate_embeddings(request: EmbeddingRequest):\n    try:\n        # Tokenize the input text and convert to tensor\n        inputs = emb_tokenizer(request.text, return_tensors=\"pt\", truncation=True, padding=True)\n\n        # Generate embeddings with BERT emb_model\n        with torch.no_grad():\n            outputs = emb_model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Averaging token embeddings\n\n        # Build response\n        response = EmbeddingResponse(\n            object=\"list\",\n            data=[\n                EmbeddingData(object=\"embedding\", embedding=embeddings, index=0)\n            ],\n            emb_model=\"bert-base-uncased\"\n        )\n\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1725517412228,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"XNnD3GtyiEpL","outputId":"4840ff9f-2b2a-4b62-dddd-e9693b038c0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PORT = 8089\n\n!curl ipv4.icanhazip.com\n!uvicorn main:app --host 0.0.0.0 --port {PORT} & lt --port {PORT} --subdomain 'myuniquesubdomain'","metadata":{"id":"t-dMS2KiadWq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Flask demo","metadata":{"id":"UhnbZxZyh7Am"}},{"cell_type":"code","source":"#@title Simple example flask\n\n%%writefile main.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\", methods=['GET'])\ndef hello():\n    return \"Hello World!\"","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1725516672674,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"qnhBzhamf3Qi","outputId":"d41f1c3d-6831-43b6-a83e-7db485138a94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PORT = 8081\n\n!curl ipv4.icanhazip.com\n!uvicorn main:app --host 0.0.0.0 --port {PORT} & lt port {PORT}","metadata":{"id":"5kk8IZqew8ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Completions API Flask\n\nfrom flask import Flask, request, jsonify\nfrom pydantic import BaseModel, ValidationError\nfrom transformers import pipeline\nfrom flask_pydantic import validate\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load the Hugging Face GPT-2 model\nmodel = pipeline(\"text-generation\", model=\"gpt2\")\n\n# Define the request model using Pydantic\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model using Pydantic\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.route(\"/v1/completions\", methods=[\"POST\"])\n@validate(body=CompletionRequest)\ndef generate_completion():\n    try:\n        # Parse request data\n        body = request.get_json()\n        completion_request = CompletionRequest(**body)\n\n        # Generate the text completion\n        completions = model(completion_request.prompt, max_length=completion_request.max_tokens, num_return_sequences=1)\n        completion_text = completions[0][\"generated_text\"]\n\n        # Return response in JSON format\n        completion_response = CompletionResponse(completion=completion_text)\n        return jsonify(completion_response.dict())\n    except ValidationError as ve:\n        return jsonify({\"error\": str(ve)}), 400\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500","metadata":{"cellView":"form","id":"3q8HirSwfWPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flask_ngrok_example.py\n\nif __name__ == '__main__':\n    run_with_lt(app)\n    !curl ipv4.icanhazip.com\n    app.run()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6425,"status":"ok","timestamp":1725511934446,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"sAzYc9I9eVSk","outputId":"709dd89d-a537-4287-9ae4-c9c21371130e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LLM demo","metadata":{"id":"o_5Yks3CmiUf"}},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers triton\n\n!pip-autoremove torch -y\n!pip install torch xformers triton\n\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n!pip install sentence-transformers\n!pip install einops\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# import os\n# os._exit(00) # Перезапустить ядро","metadata":{"execution":{"iopub.status.busy":"2024-09-09T11:33:30.433626Z","iopub.execute_input":"2024-09-09T11:33:30.434025Z","iopub.status.idle":"2024-09-09T11:37:48.164018Z","shell.execute_reply.started":"2024-09-09T11:33:30.433983Z","shell.execute_reply":"2024-09-09T11:37:48.16217Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#@title Load model\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name = \"unsloth/Phi-3.5-mini-instruct\",\n#     model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n#     model_name=\"unsloth/gemma-2-9b-bnb-4bit\",\n    model_name=\"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":831,"referenced_widgets":["ca4564b5a43942bd965b71c95d590597","a9bf870657ad4f29b367fab27d5c2b05","95f8b103f2bf4464b4d1c4d118d39ba3","001019753f6248fa986d63a83d1d5c41","76fef40758f347749cd41e4c5f5c6806","4103a592f7ce486881c92df0b1b4bf59","854cc2b642ca48b39b819d5abd5a2325","63d15564355641c0ba483f874efb4c9d","d1f2d05c25314520a536f26c1c19d91b","1fca0cdd7d374c038a143e45fcc4389a","c4df12f6133245a8bba7ec3a5de8c961","5ecccaefd21d4d859602dad6b9d125a5","e62d7ad9172a4729a9ec542799538abf","0c40b87805864dae85271a2bfb796b4b","b2032d9858d246e99a0ea29e2f1efde3","a1b816f74f1a485f85a625aff9e4bacb","1d50128505764b23b9b5d22d52a97d31","df1cdd4590e44529aa8d29e06dbe0bb8","a35893825855413b9656aec3cd121cb6","43986b4278414592aba2e451c9641341","5928858b28ad49c19565e063be6c560e","52af6bc3ca3b40288523cdf78381cb2c","b20b8fe06ece4914a5553dd51c37acd9","b4ce2532cf084b46b168a40a0f5d15ed","8e160d276dbf431d9e4ce7f8e2518449","6784362e34eb43db9804d2dd45524919","abf2c721029443ffabddd0d4fc9a4da7","279d537488d04891ac54b1d675229a59","6d3c1527050442e48c66a2c42080f2a4","f9bbf62252af4773b107d10e78473afa","e38298ed4b074caaa6f7c27f14be0fce","a76113a3890847a0ad85bbdfe0def196","870322f0245e4d9292a6465de63c1921","180bd301be244be6bbe8c8918d604af3","819eabbaf741480fb81f41fffeb501f6","d93ed850b38e4ccab33ce9500125dcd6","b055c16044e54386b3183b39c42ba799","c63f178a66314f899b1ae18476ffdae3","3cf3526fbf914e098afb230901561488","8c09fc35efa143a9a11525b395a3f88a","981aef0d94534de6a82075613a922003","2b114a7755a040beaeda35eef5c0da72","03ad52d8f3ef4b718e89769b5108c94d","a747d2a12131409d878c3c4cc7b7784c","b5b11c66daf240458a57dc3ffbe2af68","b10a9074dc6e40f1b0c5c95924c9fda4","aae76f6904a143418f543f203157fcf2","dfd6072be8564dfa90aa855dd13b102b","ec3299f897314caf9c311aed4f2db96e","86c7120aa89f4a6088c8c6c2d76c9433","f08d7b649454463aaafc8aca72c8d33b","2b0d26ade751478cb945fc1fc3f135a4","979877e9ba5146e7a5f44712f7c097e8","83c6ef2f156c497bb798ced4788c04bc","50d877d0dc2749d28182fbdb1feb995a","983862c1baa846628d10626f5a155f23","90f6a316e81e4cbb8acbc19bc6ca5d24","e49bf33368a34567ad49252465c08254","c75d620fb57f444c85a19b70656868a6","402b33690b224bd39378de95217eec4c","3a3b6779ff724132b269f041ef3f3e8f","63b261dc523a428293629e96131ec27b","1e4ee804eb2c4e7ea04d163788feeeb5","088669a32de34146a35628928a65d89e","2a5ab00f7c044526b743dbe5ac6a74d6","d9e760c2582c482087e1beca1a619808","ecfd957a14414138a6e9c982f77a6a91","5614963e84d34a309b859fa79f206185","01973e310c2842e1b031167bc7db6a80","aaaa6f8f6df64842b134d95a0466fcec","b7badbc284744462bd4f06643d5365e5","e71684a896d545acaced19f451350eb0","346f7421e84f4ef29988a63005077afa","683478ba40a04e0788c827eb70a793d1","e0650820a3d144f4ae73dca45b41dca4","4ec62ac2e2014d9c8a0bf2b320083778","59ea8fe6a0d74cc8986dbd9917e2eaac"]},"id":"K58vF08ZKYSa","outputId":"779fec18-7750-4847-c802-6c16fb7f8ce9","executionInfo":{"status":"ok","timestamp":1725533466706,"user_tz":-180,"elapsed":83086,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1725480712265,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"2ejIt2xSNKKp","outputId":"40cc3061-961d-4b48-8135-3bdf08d9502a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\n\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nprompt = alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n\nprompt = \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8\"\n# prompt = \"Why is sky blue?\"\n","metadata":{"id":"9nUs_rkjR60F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>.\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n##\n(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n##\n(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: п»їThe Project Gutenberg eBook of A Christmas Carol\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n######################\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8...\nStop when you generate exactly 10 terms of a sequence.\nWhen finished, output <|COMPLETE|>. \nFibonnaci sequence:\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If you have included (source_entity, target_entity), then do not include (target_entity, source_entity).\n\nprompt_tuned_list = [\n    '-General instructions-',\n#     'Be brief.',\n#     'Do NOT generate any code.',\n#     'Do not generate any comments.',\n#     'Do not generate any notes.'\n#     'Do not generate time.'\n]\n\nprompt_tuned = '\\n'.join(prompt_tuned_list)\n\n# prompt = prompt_tuned + '\\n' + prompt\n# prompt[:len(prompt_tuned)+30]\nprompt = prompt + prompt_tuned\nprompt[-len(prompt_tuned)-30:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=400,\n    temperature=1,\n#     do_sample=True, \n#     top_p=1.0,\n    repetition_penalty=1,\n)\n\ncompletions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\ncompletion_text = completions[0][len(prompt):]\nprint(completion_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"dUVyR85ikLrQ","executionInfo":{"status":"ok","timestamp":1725529527763,"user_tz":-180,"elapsed":3343,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"}},"outputId":"bf5f04ef-a53c-4d36-8881-3798c1c97fc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LLM Flask API","metadata":{"id":"4m4ZFjUviR79"}},{"cell_type":"code","source":"#@title Completions API Flask\n\nfrom flask import Flask, request, jsonify\nfrom pydantic import BaseModel, ValidationError\nfrom transformers import pipeline\nfrom flask_pydantic import validate\nfrom flask_lt import run_with_lt\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Define the request model using Pydantic\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model using Pydantic\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.route(\"/v1/completions\", methods=[\"POST\"])\n@validate(body=CompletionRequest)\ndef generate_completion():\n    try:\n        # Parse request data\n        body = request.get_json()\n        completion_request = CompletionRequest(**body)\n\n        # # Generate the text completion\n        # completions = model(completion_request.prompt, max_length=completion_request.max_tokens, num_return_sequences=1)\n        # completion_text = completions[0][\"generated_text\"]\n\n        inputs = tokenizer(completion_request.prompt, return_tensors = \"pt\").to(\"cuda\")\n        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n        completions = tokenizer.batch_decode(outputs)\n        completion_text = completions[0]\n\n        # Return response in JSON format\n        completion_response = CompletionResponse(completion=completion_text)\n        return jsonify(completion_response.dict())\n    except ValidationError as ve:\n        return jsonify({\"error\": str(ve)}), 400\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500","metadata":{"cellView":"form","id":"4uP9ZdYNirNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Embeddings API Flask\n\n%%writefile main.py\n\nfrom flask import Flask, request, jsonify\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Initialize Flask app\n# app = Flask(__name__)\n\n# Load BERT emb_tokenizer and emb_model (a small BERT emb_model)\nemb_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nemb_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# emb_tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n# emb_model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct')\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\ndef generate_embeddings():\n    try:\n        # Parse the incoming JSON request\n        data = request.json\n        text = data.get(\"text\", \"\")\n\n        # Tokenize the input text and convert to tensor\n        inputs = emb_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n        # Generate embeddings with BERT emb_model\n        with torch.no_grad():\n            outputs = emb_model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Averaging token embeddings\n\n        # Build response\n        response = {\n            \"object\": \"list\",\n            \"data\": [\n                {\n                    \"object\": \"embedding\",\n                    \"embedding\": embeddings,\n                    \"index\": 0\n                }\n            ],\n            \"emb_model\": \"bert-base-uncased\"\n        }\n\n        return jsonify(response)\n\n    except Exception as e:\n        # Handle exceptions and return a 500 error with the exception message\n        return jsonify({\"error\": str(e)}), 500","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1725511732341,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"40CAmqGwRAmH","outputId":"cb3df5cb-be4c-4684-f42a-e466cc8c2ff5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Completions + Embeddings API Flask\n\n%%writefile main.py\n\nfrom flask import Flask, request, jsonify\nfrom pydantic import BaseModel, ValidationError\nfrom transformers import pipeline\nfrom flask_pydantic import validate\nfrom flask_lt import run_with_lt\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Define the request model using Pydantic\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model using Pydantic\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.route(\"/v1/completions\", methods=[\"POST\"])\n@validate(body=CompletionRequest)\ndef generate_completion():\n    try:\n        # Parse request data\n        body = request.get_json()\n        completion_request = CompletionRequest(**body)\n\n        # # Generate the text completion\n        # completions = model(completion_request.prompt, max_length=completion_request.max_tokens, num_return_sequences=1)\n        # completion_text = completions[0][\"generated_text\"]\n\n        inputs = tokenizer(completion_request.prompt, return_tensors = \"pt\").to(\"cuda\")\n        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n        completions = tokenizer.batch_decode(outputs)\n        completion_text = completions[0]\n\n        # Return response in JSON format\n        completion_response = CompletionResponse(completion=completion_text)\n        return jsonify(completion_response.dict())\n    except ValidationError as ve:\n        return jsonify({\"error\": str(ve)}), 400\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n# Load BERT emb_tokenizer and emb_model (a small BERT emb_model)\nemb_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nemb_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# emb_tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n# emb_model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct')\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\ndef generate_embeddings():\n    try:\n        # Parse the incoming JSON request\n        data = request.json\n        text = data.get(\"text\", \"\")\n\n        # Tokenize the input text and convert to tensor\n        inputs = emb_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n        # Generate embeddings with BERT emb_model\n        with torch.no_grad():\n            outputs = emb_model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Averaging token embeddings\n\n        # Build response\n        response = {\n            \"object\": \"list\",\n            \"data\": [\n                {\n                    \"object\": \"embedding\",\n                    \"embedding\": embeddings,\n                    \"index\": 0\n                }\n            ],\n            \"emb_model\": \"bert-base-uncased\"\n        }\n\n        return jsonify(response)\n\n    except Exception as e:\n        # Handle exceptions and return a 500 error with the exception message\n        return jsonify({\"error\": str(e)}), 500","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1725511881443,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"XqVurdq0hLpW","outputId":"657c544c-34d6-4bcd-eea9-1c10e0e0e154"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Run app\nfrom flask import Flask\nfrom flask_lt import run_with_lt\n\nif __name__ == '__main__':\n    !curl ipv4.icanhazip.com\n\n    run_with_lt(app, subdomain='myuniquesubdomain')\n    app.run()","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137290,"status":"ok","timestamp":1725480890691,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"Ld76ed8AirNK","outputId":"1e5f46db-b3de-471e-bf86-f2b38e7ca3f7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LLM FastAPI API","metadata":{"id":"Qlykf-1O02Jc"}},{"cell_type":"code","source":"%%writefile main.py\n#@title Completion LLM FastAPI\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name = \"unsloth/Phi-3.5-mini-instruct\",\n    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Define the request model\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.post(\"/v1/completions\", response_model=CompletionResponse)\nasync def generate_completion(completion_request: CompletionRequest):\n    try:\n        # Generate the text completion\n        inputs = tokenizer(completion_request.prompt, return_tensors = \"pt\").to(\"cuda\")\n        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n        completions = tokenizer.batch_decode(outputs)\n        completion_text = completions[0]\n\n        return CompletionResponse(completion=completion_text)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"colab":{"background_save":true},"id":"ycprX1cf5C1c","outputId":"e02a51f9-e67a-41af-8e58-ac473989d085","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Chat completion FastAPI\n\n%%writefile main.py\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\nmax_seq_length = 2048\ndtype = torch.float16\nload_in_4bit = True\n\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n    # ... other models ...\n]\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Phi-3.5-mini-instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nFastLanguageModel.for_inference(model)\n\n# Define the message structure for the chat-based interaction\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n# Define the request model for chat-based completion\nclass ChatCompletionRequest(BaseModel):\n    messages: list[ChatMessage]\n    max_tokens: int = 50\n    n: int = 1\n\n# Define the response model for chat-based completion\nclass ChatCompletionResponse(BaseModel):\n    completions: list[str]\n\nimport logging\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def generate_chat_completion(chat_request: ChatCompletionRequest):\n    print(f\"REQUEST: {chat_request}\")\n    try:\n        # Convert the chat messages into a single prompt\n        prompt = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in chat_request.messages])\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=chat_request.max_tokens,\n            do_sample=True,          # Enable sampling\n#             top_p=0.9,               # Set nucleus sampling threshold\n            num_return_sequences=chat_request.n  # Generate n completions\n        )\n        completion_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        # Return all completions as a list\n        return ChatCompletionResponse(completions=completion_texts)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1725530466090,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"JLGjs3BPWLr-","outputId":"cdd401f4-45c7-4207-c29a-367959cb50a5","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title [OAI] Chat Completion LLM FastAPI\n\n%%writefile main.py\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Define the message structure for the chat-based interaction\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n# Define the request model for chat-based completion\nclass ChatCompletionRequest(BaseModel):\n    messages: list[ChatMessage]\n    max_tokens: int = 50\n    n: int = 1\n\n# Define the response model for chat-based completion\nclass ChatCompletionChoice(BaseModel):\n    message: ChatMessage\n    index: int\n\nclass ChatCompletionResponse(BaseModel):\n    id: str\n    object: str\n    created: int\n    model: str\n    choices: list[ChatCompletionChoice]\n    usage: dict\n\nimport logging\nimport uuid\nimport time\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def generate_chat_completion(chat_request: ChatCompletionRequest):\n    print(f\"REQUEST: {chat_request}\")\n    try:\n        # Convert the chat messages into a single prompt\n        prompt = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in chat_request.messages])\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=chat_request.max_tokens,\n            do_sample=True,          # Enable sampling\n#             top_p=0.9,               # Set nucleus sampling threshold\n            num_return_sequences=chat_request.n  # Generate n completions\n        )\n        completion_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        # Create response according to OpenAI Chat Completions format\n        response = ChatCompletionResponse(\n            id=str(uuid.uuid4()),  # Generate a unique ID for the response\n            object=\"chat.completion\",\n            created=int(time.time()),  # Current timestamp\n            model=\"your-model-id\",  # Replace with your actual model ID\n            choices=[\n                ChatCompletionChoice(\n                    message=ChatMessage(role=\"assistant\", content=text),\n                    index=i\n                )\n                for i, text in enumerate(completion_texts)\n            ],\n            usage={\n                \"prompt_tokens\": len(inputs[\"input_ids\"][0]),\n                \"completion_tokens\": sum(len(tokenizer.encode(text)) for text in completion_texts),\n                \"total_tokens\": len(inputs[\"input_ids\"][0]) + sum(len(tokenizer.encode(text)) for text in completion_texts)\n            }\n        )\n\n        return response\n    except Exception as e:\n        logger.error(f\"Error occurred: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"_z2w-oFizeU2","executionInfo":{"status":"ok","timestamp":1725533582391,"user_tz":-180,"elapsed":427,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"bdf8016f-e707-44a8-bad7-ba56bebb2c6f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Embeddings API\n\n%%writefile main.py\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Load BERT emb_tokenizer and emb_model (a small BERT emb_model)\nemb_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nemb_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Define the request emb_model\nclass EmbeddingRequest(BaseModel):\n    text: str\n\n# Define the response emb_model\nclass EmbeddingData(BaseModel):\n    object: str\n    embedding: list\n    index: int\n\nclass EmbeddingResponse(BaseModel):\n    object: str\n    data: list[EmbeddingData]\n    emb_model: str\n\n@app.post(\"/v1/embeddings\", response_model=EmbeddingResponse)\nasync def generate_embeddings(request: EmbeddingRequest):\n    try:\n        # Tokenize the input text and convert to tensor\n        inputs = emb_tokenizer(request.text, return_tensors=\"pt\", truncation=True, padding=True)\n\n        # Generate embeddings with BERT emb_model\n        with torch.no_grad():\n            outputs = emb_model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Averaging token embeddings\n\n        # Build response\n        response = EmbeddingResponse(\n            object=\"list\",\n            data=[\n                EmbeddingData(object=\"embedding\", embedding=embeddings, index=0)\n            ],\n            emb_model=\"bert-base-uncased\"\n        )\n\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1725524916580,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"QW_dw1grTBoy","outputId":"1d34228a-c7d6-4e07-8bed-87259c7c8569","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Completion + Embeddings + Chat LLM FastAPI\n\n%%writefile main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\nfrom flask import Flask, request, jsonify\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\nfrom unsloth import FastLanguageModel\nimport torch\n\n# Initialize FastAPI app\napp = FastAPI()\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# Define the request model\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.post(\"/v1/completions\", response_model=CompletionResponse)\nasync def generate_completion(completion_request: CompletionRequest):\n    try:\n        # Generate the text completion\n        inputs = tokenizer(completion_request.prompt, return_tensors = \"pt\").to(\"cuda\")\n        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n        completions = tokenizer.batch_decode(outputs)\n        completion_text = completions[0]\n\n        return CompletionResponse(completion=completion_text)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Load BERT emb_tokenizer and emb_model (a small BERT emb_model)\nemb_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nemb_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Define the request emb_model\nclass EmbeddingRequest(BaseModel):\n    text: str\n\n# Define the response emb_model\nclass EmbeddingData(BaseModel):\n    object: str\n    embedding: list\n    index: int\n\nclass EmbeddingResponse(BaseModel):\n    object: str\n    data: list[EmbeddingData]\n    emb_model: str\n\n@app.post(\"/v1/embeddings\", response_model=EmbeddingResponse)\nasync def generate_embeddings(request: EmbeddingRequest):\n    try:\n        # Tokenize the input text and convert to tensor\n        inputs = emb_tokenizer(request.text, return_tensors=\"pt\", truncation=True, padding=True)\n\n        # Generate embeddings with BERT emb_model\n        with torch.no_grad():\n            outputs = emb_model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Averaging token embeddings\n\n        # Build response\n        response = EmbeddingResponse(\n            object=\"list\",\n            data=[\n                EmbeddingData(object=\"embedding\", embedding=embeddings, index=0)\n            ],\n            emb_model=\"bert-base-uncased\"\n        )\n\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Define the message structure for the chat-based interaction\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n# Define the request model for chat-based completion\nclass ChatCompletionRequest(BaseModel):\n    messages: list[ChatMessage]\n    max_tokens: int = 50\n\n# Define the response model for chat-based completion\nclass ChatCompletionResponse(BaseModel):\n    completion: str\n\nimport logging\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def generate_chat_completion(chat_request: ChatCompletionRequest):\n    print(f\"REQUEST: {chat_request}\")\n    try:\n        # Convert the chat messages into a single prompt\n        prompt = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in chat_request.messages])\n\n        # Generate the text completion\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(**inputs, max_new_tokens=chat_request.max_tokens, use_cache=True)\n        completion_text = tokenizer.batch_decode(outputs)[0]\n\n        print(f\"RESPONSE: {completion_text}\")\n        return ChatCompletionResponse(completion=completion_text)\n    except Exception as e:\n        logger.error(f\"Error generating completion: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1725526877846,"user":{"displayName":"Антон Чернов","userId":"04928198919640259337"},"user_tz":-180},"id":"Q013IyHJ6MN2","outputId":"cd5f9a81-b272-4d26-efed-d8e1ded91743","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [OAI] Completion + Embeddings + Chat LLM FastAPI","metadata":{"id":"B0t0wNYJJCsg"}},{"cell_type":"markdown","source":"## [OAI] Load models","metadata":{}},{"cell_type":"code","source":"%%writefile main.py\n#@title Models loading\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\nfrom flask import Flask, request, jsonify\nfrom unsloth import FastLanguageModel\nimport torch\nfrom typing import List\nimport logging\nimport uuid\nimport time\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\n# MODEL_NAME = \"unsloth/Phi-3.5-mini-instruct\"\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\nMODEL_NAME=\"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\"\n\n# Suppress warning\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\nfor logger in loggers:\n    if \"transformers\" in logger.name.lower():\n        logger.setLevel(logging.ERROR)\n        \nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725540268989,"user_tz":-180,"elapsed":817,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"cc18c960-cd4a-46e1-e63c-1f899e6c4fdc","id":"d-E2YAdkJT91","execution":{"iopub.status.busy":"2024-09-09T10:12:07.253605Z","iopub.execute_input":"2024-09-09T10:12:07.254076Z","iopub.status.idle":"2024-09-09T10:12:07.261216Z","shell.execute_reply.started":"2024-09-09T10:12:07.254039Z","shell.execute_reply":"2024-09-09T10:12:07.260185Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# # %%writefile -a main.py\n\n# from transformers import BertTokenizer, BertModel\n# from transformers import AutoTokenizer, AutoModel\n# from transformers import BertTokenizer, BertModel\n\n# EMB_MODEL_NAME = \"bert-base-uncased\"\n# emb_tokenizer = BertTokenizer.from_pretrained(EMB_MODEL_NAME)\n# emb_model = BertModel.from_pretrained(EMB_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:07.479285Z","iopub.execute_input":"2024-09-09T10:12:07.479606Z","iopub.status.idle":"2024-09-09T10:12:07.484086Z","shell.execute_reply.started":"2024-09-09T10:12:07.479567Z","shell.execute_reply":"2024-09-09T10:12:07.482941Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# input_texts = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\n\n# text = input_texts[0]\n# encoded_input = emb_tokenizer(text, return_tensors='pt')\n# output = emb_model(**encoded_input)\n# embedding = output.last_hidden_state.mean(dim=1).squeeze().tolist()\n# len(embedding)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:07.658263Z","iopub.execute_input":"2024-09-09T10:12:07.658589Z","iopub.status.idle":"2024-09-09T10:12:07.662374Z","shell.execute_reply.started":"2024-09-09T10:12:07.658556Z","shell.execute_reply":"2024-09-09T10:12:07.66145Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"%%writefile -a main.py\n\nfrom sentence_transformers import SentenceTransformer\n\nEMB_MODEL_NAME = \"nomic-ai/nomic-embed-text-v1.5\"\nemb_model = SentenceTransformer(EMB_MODEL_NAME, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:07.850451Z","iopub.execute_input":"2024-09-09T10:12:07.850771Z","iopub.status.idle":"2024-09-09T10:12:07.857535Z","shell.execute_reply.started":"2024-09-09T10:12:07.850739Z","shell.execute_reply":"2024-09-09T10:12:07.856656Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# input_texts = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\n\n# embeddings = emb_model.encode(input_texts)\n# len(embeddings[0])\n# embeddings[0].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:08.040618Z","iopub.execute_input":"2024-09-09T10:12:08.040903Z","iopub.status.idle":"2024-09-09T10:12:08.044905Z","shell.execute_reply.started":"2024-09-09T10:12:08.040859Z","shell.execute_reply":"2024-09-09T10:12:08.043994Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"## [OAI] Completions","metadata":{}},{"cell_type":"code","source":"%%writefile -a main.py\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# # Configure basic logging\n# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n# logger = logging.getLogger(__name__)\n\n# Define the request model\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model for completion\nclass CompletionChoice(BaseModel):\n    text: str\n    index: int\n\nclass CompletionResponse(BaseModel):\n    id: str\n    object: str\n    created: int\n    model: str\n    choices: List[CompletionChoice]\n    usage: dict\n\n@app.post(\"/v1/completions\", response_model=CompletionResponse)\nasync def generate_completion(completion_request: CompletionRequest):\n    print(f\"\\n\\nREQUEST:\")\n    print(f\"{completion_request.prompt}\")\n    \n    try:\n        # Generate the text completion\n        inputs = tokenizer(completion_request.prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=completion_request.max_tokens,\n            do_sample=True,\n            top_p=0.9\n        )\n    \n        completions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        completion_start = len(completion_request.prompt)\n        completions = [comp[completion_start:] for comp in completions]\n\n        response = CompletionResponse(\n            id=str(uuid.uuid4()),  # Unique ID for the completion\n            object=\"text_completion\",\n            created=int(time.time()),  # Current timestamp\n            model=MODEL_NAME,  # Model used\n            choices=[\n                CompletionChoice(\n                    text=text,\n                    index=i\n                )\n                for i, text in enumerate(completions)\n            ],\n            usage={\n                \"prompt_tokens\": len(inputs[\"input_ids\"][0]),\n                \"completion_tokens\": sum(len(tokenizer.encode(text)) for text in completions),\n                \"total_tokens\": len(inputs[\"input_ids\"][0]) + sum(len(tokenizer.encode(text)) for text in completions)\n            }\n        )\n        \n        print(f\"\\nRESPONSE:\")\n        print(f\"{completions[0]}\")\n        \n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"r1nKi5S1JdtM","executionInfo":{"status":"ok","timestamp":1725540269519,"user_tz":-180,"elapsed":6,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"f9094932-2033-4c57-ff9e-f5c91abc87ae","execution":{"iopub.status.busy":"2024-09-09T10:12:08.411044Z","iopub.execute_input":"2024-09-09T10:12:08.411445Z","iopub.status.idle":"2024-09-09T10:12:08.418751Z","shell.execute_reply.started":"2024-09-09T10:12:08.411408Z","shell.execute_reply":"2024-09-09T10:12:08.417853Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## [OAI] Embeddings","metadata":{}},{"cell_type":"code","source":"%%writefile -a main.py\n\nfrom typing import List, Union\nfrom typing import Optional\n\n# Define the input and response models\nclass EmbeddingRequest(BaseModel):\n    input: Union[str, List[str]]  # Required field\n    model: Optional[str] = None   # Optional field\n\nclass EmbeddingData(BaseModel):\n    object: str                   # \"embedding\"\n    embedding: List[float]        # Embedding vector\n    index: int                    # Index of the input\n\nclass EmbeddingResponse(BaseModel):\n    object: str                   # \"list\"\n    data: List[EmbeddingData]     # List of embedding data\n    model: str                    # Model name\n\n@app.post(\"/v1/embeddings\", response_model=EmbeddingResponse)\nasync def generate_embeddings(request: EmbeddingRequest):\n    try:\n        input_texts = [request.input] if isinstance(request.input, str) else request.input\n        \n        # MODEL CALL INTERFACE: Tokenizer + Model\n#         responses = []\n#         for idx, text in enumerate(input_texts):\n#             inputs = emb_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n#             with torch.no_grad():\n#                 outputs = emb_model(**inputs)\n#                 embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n\n#             responses.append(EmbeddingData(object=\"embedding\", embedding=embedding, index=idx))\n\n        # MODEL CALL INTERFACE: Model only (nomic-ai)\n        input_texts = ['search_document: ' + txt for txt in input_texts]\n        embeddings = emb_model.encode(input_texts).tolist()\n        responses = [\n            EmbeddingData(object=\"embedding\", embedding=embedding, index=idx)\n            for idx, embedding in enumerate(embeddings)\n        ]\n    \n        return EmbeddingResponse(\n            object=\"list\",\n            data=responses,\n            model=request.model or EMB_MODEL_NAME  # Use default model name if not provided\n        )\n\n    except Exception as e:\n        # Handle and log unexpected errors\n        print(f\"Error: {str(e)}\")\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"TI1CVdUsJzsT","executionInfo":{"status":"ok","timestamp":1725540269520,"user_tz":-180,"elapsed":6,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"346d2842-d272-494f-c62a-42783d3640e1","execution":{"iopub.status.busy":"2024-09-09T10:12:08.79734Z","iopub.execute_input":"2024-09-09T10:12:08.798003Z","iopub.status.idle":"2024-09-09T10:12:08.804847Z","shell.execute_reply.started":"2024-09-09T10:12:08.797963Z","shell.execute_reply":"2024-09-09T10:12:08.803885Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## [OAI] Chat completions","metadata":{}},{"cell_type":"code","source":"%%writefile -a main.py\n\n# Define the message structure for the chat-based interaction\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n# Define the request model for chat-based completion\nclass ChatCompletionRequest(BaseModel):\n    messages: List[ChatMessage]\n    max_tokens: int = 50\n\n# Define the response model for chat-based completion\nclass ChatCompletionChoice(BaseModel):\n    message: ChatMessage\n    index: int\n\nclass ChatCompletionResponse(BaseModel):\n    id: str\n    object: str\n    created: int\n    model: str\n    choices: List[ChatCompletionChoice]\n    usage: dict","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"lWyJhfMEKBfL","executionInfo":{"status":"ok","timestamp":1725540269955,"user_tz":-180,"elapsed":3,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"6c480e65-755d-4419-d0a4-704ded984cee","execution":{"iopub.status.busy":"2024-09-09T10:12:09.527879Z","iopub.execute_input":"2024-09-09T10:12:09.529005Z","iopub.status.idle":"2024-09-09T10:12:09.535434Z","shell.execute_reply.started":"2024-09-09T10:12:09.52896Z","shell.execute_reply":"2024-09-09T10:12:09.534404Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a main.py\n\n# Chat completions: non-streaming\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def generate_chat_completion(chat_request: ChatCompletionRequest):\n    print(f\"\\n\\nREQUEST:\")\n#     print(chat_request)\n    for k, msg in enumerate(chat_request.messages):\n            print(f\"({msg.role}): {msg.content}\")\n\n    try:\n                  \n        # Convert the chat messages into a single prompt\n        prompt = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in chat_request.messages])\n#         prompt += \" Respond with your own thoughts, do not complete the user's input.\"\n        prompt += \"\\nassistant:\"\n        # Generate the text completion\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        \n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=chat_request.max_tokens,\n            do_sample=True,          # Enable sampling\n            top_p=0.9,               # Set nucleus sampling threshold\n            num_return_sequences=1  # Generate a single completion\n        )\n        \n        \n        completion_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        completion_start = len(prompt)\n        completion_texts = [comp[completion_start:] for comp in completion_texts]\n        \n        print(f'completion_texts len: {len(completion_texts)}')\n        print(f'completion_start: {completion_start}')\n        \n        # Create response according to OpenAI Chat Completions format\n        response = ChatCompletionResponse(\n            id=str(uuid.uuid4()),  # Generate a unique ID for the response\n            object=\"chat.completion\",\n            created=int(time.time()),  # Current timestamp\n            model=MODEL_NAME,  # Model used\n            choices=[\n                ChatCompletionChoice(\n                    message=ChatMessage(role=\"assistant\", content=text),\n                    index=i\n                )\n                for i, text in enumerate(completion_texts)\n            ],\n            usage={\n                \"prompt_tokens\": len(inputs[\"input_ids\"][0]),\n                \"completion_tokens\": sum(len(tokenizer.encode(text)) for text in completion_texts),\n                \"total_tokens\": len(inputs[\"input_ids\"][0]) + sum(len(tokenizer.encode(text)) for text in completion_texts)\n            }\n        )\n\n        res = response.choices[0]\n        print(f\"\\nRESPONSE:\")\n        print(f\"({res.message.role}): {res.message.content}\")\n              \n        return response\n    except Exception as e:\n        print(f\"Error generating completion: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:09.711658Z","iopub.execute_input":"2024-09-09T10:12:09.712077Z","iopub.status.idle":"2024-09-09T10:12:09.721154Z","shell.execute_reply.started":"2024-09-09T10:12:09.712036Z","shell.execute_reply":"2024-09-09T10:12:09.719799Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# %%writefile -a main.py\n\n# # Chat completions: streaming = True\n\n# from fastapi.responses import StreamingResponse\n# from sse_starlette.sse import EventSourceResponse\n# import uuid\n# import json\n# import time\n\n# @app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\n# async def generate_chat_completion(chat_request: ChatCompletionRequest):\n#     print(f\"\\n\\nREQUEST:\")\n#     for k, msg in enumerate(chat_request.messages):\n#         print(f\"({msg.role}): {msg.content}\")\n\n#     try:\n#         # Convert the chat messages into a single prompt\n#         prompt = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in chat_request.messages])\n#         prompt += \"\\nassistant:\"\n\n#         # Tokenize the prompt\n#         inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        \n#         if chat_request.streaming:\n#             # Define a generator function to yield tokens as they're generated\n#             async def token_generator(prompt, max_tokens):\n#                 # Tokenize the prompt\n#                 input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n#                 # Initialize generated tokens with the prompt\n#                 generated_tokens = input_ids\n#                 total_generated_tokens = 0\n\n#                 # Manually generate tokens one by one\n#                 for _ in range(max_tokens):\n#                     with torch.no_grad():\n#                         # Generate the next token\n#                         outputs = model(input_ids=generated_tokens)\n#                         next_token_logits = outputs.logits[:, -1, :]\n#                         next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n\n#                         # Append the generated token to the sequence\n#                         generated_tokens = torch.cat((generated_tokens, next_token_id), dim=1)\n\n#                         # Decode and get the latest token generated\n#                         next_token_text = tokenizer.decode(next_token_id[0], clean_up_tokenization_spaces=True)\n\n#                         # Increment total generated tokens\n#                         total_generated_tokens += 1\n\n#                         print(next_token_text, end=\"\")\n#                         # Ensure we're not yielding empty strings\n#                         if next_token_text.strip():\n#                             # Yield the token in OpenAI API's streaming format\n#                             yield f'data: {json.dumps({\"choices\": [{\"delta\": {\"content\": next_token_text}, \"index\": 0, \"finish_reason\": None}]})}\\n\\n'\n\n#                         # Stop generation if the EOS token is encountered\n#                         if next_token_id.item() == tokenizer.eos_token_id:\n#                             yield f'data: {json.dumps({\"choices\": [{\"delta\": {}, \"index\": 0, \"finish_reason\": \"stop\"}]})}\\n\\n'\n#                             break\n\n#                 # If max_tokens is reached without an EOS token, send the finish event\n#                 yield f'data: {json.dumps({\"choices\": [{\"delta\": {}, \"index\": 0, \"finish_reason\": \"length\"}]})}\\n\\n'\n\n\n#             print(f\"\\nRESPONSE:\\n(assistant):\")\n#             # Return a streaming response\n#             return EventSourceResponse(token_generator(prompt, chat_request.max_tokens))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:09.887772Z","iopub.execute_input":"2024-09-09T10:12:09.88844Z","iopub.status.idle":"2024-09-09T10:12:09.895221Z","shell.execute_reply.started":"2024-09-09T10:12:09.888402Z","shell.execute_reply":"2024-09-09T10:12:09.894161Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"# %%writefile -a main.py\n\n#     # Chat completions: streaming = False\n#     else:\n#         outputs = model.generate(\n#             **inputs,\n#             max_new_tokens=chat_request.max_tokens,\n#             do_sample=True,          # Enable sampling\n#             top_p=0.9,               # Set nucleus sampling threshold\n#             num_return_sequences=1  # Generate a single completion\n#         )\n        \n        \n#         completion_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n#         completion_start = len(prompt)\n#         completion_texts = [comp[completion_start:] for comp in completion_texts]\n        \n#         print(f'completion_texts len: {len(completion_texts)}')\n#         print(f'completion_start: {completion_start}')\n        \n#         # Create response according to OpenAI Chat Completions format\n#         response = ChatCompletionResponse(\n#             id=str(uuid.uuid4()),  # Generate a unique ID for the response\n#             object=\"chat.completion\",\n#             created=int(time.time()),  # Current timestamp\n#             model=MODEL_NAME,  # Model used\n#             choices=[\n#                 ChatCompletionChoice(\n#                     message=ChatMessage(role=\"assistant\", content=text),\n#                     index=i\n#                 )\n#                 for i, text in enumerate(completion_texts)\n#             ],\n#             usage={\n#                 \"prompt_tokens\": len(inputs[\"input_ids\"][0]),\n#                 \"completion_tokens\": sum(len(tokenizer.encode(text)) for text in completion_texts),\n#                 \"total_tokens\": len(inputs[\"input_ids\"][0]) + sum(len(tokenizer.encode(text)) for text in completion_texts)\n#             }\n#         )\n\n#         res = response.choices[0]\n#         print(f\"\\nRESPONSE:\")\n#         print(f\"({res.message.role}): {res.message.content}\")\n              \n#         return response\n#     except Exception as e:\n#         print(f\"Error generating completion: {e}\")\n#         raise HTTPException(status_code=500, detail=str(e))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:10.418138Z","iopub.execute_input":"2024-09-09T10:12:10.419023Z","iopub.status.idle":"2024-09-09T10:12:10.424601Z","shell.execute_reply.started":"2024-09-09T10:12:10.418986Z","shell.execute_reply":"2024-09-09T10:12:10.423606Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"%%writefile -a main.py\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\n\n# Exception handler to log invalid requests\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    print(f\"Invalid request: {request}\")\n    print(f\"Validation error: {exc.errors()}\")\n    return JSONResponse(\n        status_code=422,\n        content={\"detail\": exc.errors()},\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-09T10:12:10.661456Z","iopub.execute_input":"2024-09-09T10:12:10.662184Z","iopub.status.idle":"2024-09-09T10:12:10.667892Z","shell.execute_reply.started":"2024-09-09T10:12:10.662146Z","shell.execute_reply":"2024-09-09T10:12:10.666925Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Appending to main.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Launch API","metadata":{"id":"qN8KVu_oI_f3"}},{"cell_type":"code","source":"PORT = 8089\n\n!curl ipv4.icanhazip.com\n!uvicorn main:app --host 0.0.0.0 --port {PORT} & lt --port {PORT} --subdomain 'very-unique-sub-domain'","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Qrx3P-kl5_Rz","executionInfo":{"status":"error","timestamp":1725543193658,"user_tz":-180,"elapsed":17556,"user":{"displayName":"Антон Чернов","userId":"15174499269838391667"}},"outputId":"ea14b0e4-35eb-4bbd-c6fe-3f296ca10e58","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\n\nREQUEST:\nHello, world!\n\nRESPONSE:\n This is the [Introduction](/introduction) page\n\u001b[32mINFO\u001b[0m:     79.139.204.44:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Experimental","metadata":{"id":"QqM0abd18es3"}},{"cell_type":"code","source":"#@title Dummy completions API Flask\n\nfrom flask import Flask, request, jsonify\nfrom pydantic import BaseModel, ValidationError\nfrom flask_pydantic import validate\nfrom flask_lt import run_with_lt\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Define the request model using Pydantic\nclass CompletionRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 50\n\n# Define the response model using Pydantic\nclass CompletionResponse(BaseModel):\n    completion: str\n\n@app.route(\"/v1/completions\", methods=[\"POST\"])\n@validate(body=CompletionRequest)\ndef generate_completion():\n    try:\n        # Parse request data\n        body = request.get_json()\n        completion_request = CompletionRequest(**body)\n\n        completion_text = \"Successul POST request.\"\n\n        # Return response in JSON format\n        completion_response = CompletionResponse(completion=completion_text)\n        return jsonify(completion_response.dict())\n    except ValidationError as ve:\n        return jsonify({\"error\": str(ve)}), 400\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500","metadata":{"cellView":"form","id":"FemzAcepsh1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Custom Local Tunnel\n\nimport time\nfrom threading import Timer\nfrom py_localtunnel.lt import run_localtunnel\nimport threading\nimport concurrent\nimport logging\n\n__version__ = \"1.0.7\"\n\nclass HaltableTimer(Timer):\n    def __init__(self, *args, name='DefaultTimer', delay=5, **kwargs,):\n        super().__init__(*args, **kwargs)\n\n        # port = kwargs.get('port', 5000)\n        # subdomain='subdomain'\n        # super().__init__(1, start_lt, args=(port, subdomain,))\n\n        self.name = name\n        self.delay = delay\n        self.halt = False\n\n    def run(self):\n        while not self.halt:\n            logging.info('Thread %s running.' %self.name)\n            time.sleep(self.delay)\n\n    def do_halt(self):\n        self.halt = True\n        self.join()\n\ndef run_lt(port: int, subdomain: str = None, local_host: str = \"127.0.0.1\"):\n    run_localtunnel(port, subdomain, local_host)\n\ndef start_lt(port: int, subdomain: str = None):\n    lt_adress = run_lt(port, subdomain)\n    print(lt_adress)\n\ndef monitor_process(process, stop_event):\n    while not stop_event.is_set():\n        if process.poll() is not None:\n            break\n        time.sleep(1)\n\n    if process.poll() is None:\n        process.terminate()\n        process.wait()\n\n# Create a stop event\nstop_event = threading.Event()\n\ndef run_with_lt(app, subdomain: str = None):\n    old_run = app.run\n\n    def new_run(*args, **kwargs):\n        port = kwargs.get('port', 5000)\n\n        # with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n        #     future = executor.submit(start_lt, port, subdomain)\n        #     process = future.result()\n        #     monitor_process(process, stop_event)\n        #     time.sleep(5)\n        #     stop_event.set()\n\n        # if process.poll() is None:\n        #     process.terminate()\n        #     process.wait()\n\n        thread = Timer(1, start_lt, args=(port, subdomain,))\n        thread.setDaemon(True)\n        thread.start()\n\n        # stop_event = threading.Event()\n        # time.sleep(5)\n        # stop_event.set()\n        # thread.join()\n\n        old_run(*args, **kwargs)\n    app.run = new_run","metadata":{"cellView":"form","id":"iaeLksfnredw"},"execution_count":null,"outputs":[]}]}